**Multimodal Person Discovery in Broadcast TV Task?** 
 - Very interested. I'd like to participate: 19 (50%) - Sounds quite interesting. Don't know for sure if I'd participate: 11 (28.95%) - If I had more information, I might consider it: 3 (7.89%) - This task is not for me: 5 (13.16%) - Total: 38**If you have comments, add them here. Why (or why not) would you be interested in this task? Do you have suggestions for the task? Is the task innovative? Already solved? Too novel to yet be of interest? Just right to be of value to the research community? Do you know of an existing benchmark, data set or project related to this task (please specify)?** - The task seems very interesting. I am a researcher in Speaker Diarization, and hence my system would make contribution to that aspect of this multimedia task. The task is innovative in the sense that it makes use of audio and visual cues in a TV broadcast to find the required information. I dont think this particular problem has been solved with remarkable accuracy. In fact I would like to answers to this question from others taking the survey.**We are planning a data set of ca. 200 hours of material.** - This is an adequate amount: 30 (90.91%) - I would like more or less data. (Please describe your view on the data set size.: 3 (9.09%) - Total: 33
Comments:
 - I'm not sure there's a need for such an amount of data for this task. I know that "the best data is more data" but does the saying still applies to unsupervised discovery tasks? It might be a burden to develop a first system this year with such an amount of data. - The content of media should be defined clearly. At most how many people will be identified and how much time each person will be appear on average? - I think that the test corpus should not exceed more than 10 hours. It will quickly become too expensive to run the systems on big test corpus.**We are thinking of asking participants to annotate videos with people who were the source of social media buzz (e.g., according to Google Trends) when the video was first broadcast. What do you think about this idea? (Please keep in mind, it is probably not feasible to annotate all of the data.)** - We would participate in the annotation of audio material, video is not our area of expertise. - No strong opinion on the subject. I think it's better to avoid an overrepresentation of famous people in the annotations to not encourage buidling models from trendy famous people beforehand. - We might participate. - Good ideas, annotated corpus will be bigger - I don't think participants will be interested in annotating videos - acceptable if the amount of annotated data required is limited to one or two days of annotations - It is a good idea - This data set seems fairy selected, and should lead to interestinf user tests - I do not think I will be able to devote enough time for the annotation. I can annotate upto 2-3 hours or data - Seems it could be challenging to get a good groundtruth on this since getting full social media coverage isn't easy to get. - And what about the people that have been correctly detected by the systems but which do not appear in Google Trends ? - sounds reasonable**Participants are asked to provide a proof for each returned shot (e.g., a short excerpt of the test set showing the person AND its name at the same time). Do you think this will influence the design of your algorithms? If you have an opinion, please comment.** - Yes, sure: 12 (38.71%) - Maybe: 14 (45.16%) - No: 5 (16.13%) - Total: 31Comments:
 - I have an idea in mind that I'd love to benchmark but it does hardly allow for a proof. I will need to either implement something specific to get a proof or change the idea. I understand the proof has an interest for evaluation purposes but I can only think of a few use-cases where the proof would be a plus. - Will not influence my algorithms**How much time can you (or your team) devote to the a posteriori collaborative annotation?** - Unfortunately, we wouldn't have time to help with annotation.	15	46.88% - 8 hours: 9 (28.13%) - 16 hours: 5 (15.63%) - 40 hours: 3 (9.38%) - Other (please specify): 1 (3.13%) - Total: 33 
Comments:
 - Somewhere between 2 and 4 hours sounds more reasonable.Is your participation affected by the languages included in the data (French and Catalan)? Recall that automatic speech transcription, optical character recognition and named entity detection will be provided for both the training and test sets. If you have an opinion, please comment. - No - Language does affect participation. Even if transcripts, OCR and named entities are provided, my understanding of the task is that there's a need for NLP to (sort of) associate entities with images and faces. Doing so requires at the very least a bit of NLP. Not having the necessary tools for one language might be detrimental to design the system. Understanding the language is also an invaluable asset to help design the system and diagnose errors. I might not return results for languages I have no familiarity with. - This might make annotation hard, as we speak neither language - My participation is not affected at all by the languages included in the data, as we try to move towards language-independent systems - English data would be better. - No affected - no - We don't have and ASR system for catalan. if it is provided, we can work with it. - no - I am a fluent English speaker. But I have no knowledge of French or Catalan. Since my contribution is going to be in the speaker diarization part, I genuinely hope that the language barrier will not cause problems in my participation in the task. - Yes, we prefer use our systems that have been optimized to identify person on TV broadcast. We prefer to work on french data. - probably not**If any additional comments, questions or suggestions about this task occurred to you while you were answering the detailed questions, it would be helpful if you could enter them here:** - I'm not very sure what the dev data will look like. I'm seriously considering on running the task if it survives the MediaEval selection process but I cannot tell for sure before seeing the training data and corresponding annotations. - I have the following doubts. 1. System output formats. Are they text outputs. Is the text french/catalan? Is it possible to get translations in English. 2. Structure of dataset. Does the dataset consist of episodes of TV broadcasts? 3. Forum for similar area researchers. I would like to get in touch with other researchers from the speaker diarization community who are participating in this campaign. Is there a forum where all such researchers are enabled to communicate with each other. 4. Forum for different area researchers. Is there a forum where I can communicate with researchers of other research areas such as face detection/OCR etc so that I can ask them doubts if I have any. I am sure I will have doubts in the future since I have no experience in any of the remaining areas. 5. Evaluation criteria. What will the evaluation criterion be in terms of? Will all the evaluations be done separately for each module against the corresponding system in the baseline module? 6. System implementation restrictions. Is there a restriction on the language in which the implementation of our algorithm can be done. For eg: only open source software systems to be used. - Does the training corpus will be completely annotated ? 